{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pullelys/iml-project/blob/main/task_2/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1PiFkfsn90M",
    "outputId": "77022b93-e653-4c94-fcb7-fb304df0dc18"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEZWvVHpZEQI",
    "outputId": "9e176912-3cd2-4f67-b3f3-c9039afb4128"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpK_n1XGo7C_",
    "outputId": "d2ec1186-4ea9-43f4-8f32-fe1f925abc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://colab-bucket-86f9472c-3ef9-11eb-a0d2-0242ac1c0002/train_labels.csv...\n",
      "/ [1 files][891.7 KiB/891.7 KiB]                                                \n",
      "Operation completed over 1 objects/891.7 KiB.                                    \n",
      "Copying gs://colab-bucket-86f9472c-3ef9-11eb-a0d2-0242ac1c0002/train_features.csv...\n",
      "- [1 files][ 34.2 MiB/ 34.2 MiB]                                                \n",
      "Operation completed over 1 objects/34.2 MiB.                                     \n",
      "Copying gs://colab-bucket-86f9472c-3ef9-11eb-a0d2-0242ac1c0002/test_features.csv...\n",
      "/ [1 files][ 22.8 MiB/ 22.8 MiB]                                                \n",
      "Operation completed over 1 objects/22.8 MiB.                                     \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2JppC03CZEQV"
   },
   "outputs": [],
   "source": [
    "# define main file name\n",
    "main_filename = 'main'\n",
    "\n",
    "# read and sort data\n",
    "X_train = pd.read_csv('train_features.csv').sort_values(by=['pid', 'Time'])\n",
    "y_train = pd.read_csv('train_labels.csv').sort_values(by=['pid'])\n",
    "X_test = pd.read_csv('test_features.csv').sort_values(by=['pid', 'Time'])\n",
    "\n",
    "# define column names as specified in the correct submission format\n",
    "# partition them into the corresponding subtasks\n",
    "subtask_1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', \n",
    "             'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', \n",
    "             'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "subtask_2 = ['LABEL_Sepsis']\n",
    "subtask_3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "\n",
    "# initialize df_predictions with unique 'pid' column from X_test\n",
    "df_predictions = pd.DataFrame(X_test['pid'].unique(), columns=['pid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09SXF-e_eypw",
    "outputId": "4a3104b2-1823-4ea0-dd45-becfa5d36323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# perform classifier grid search\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   55.6s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  6.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed: 19.6min\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 21.9min\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 23.4min finished\n",
      "# perform classifier grid search: runtime: 0 h 24 min 4.44 s\n",
      "# make predictions\n",
      "# summarize GridSearchCV results\n",
      "0.817046 (0.004062) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
      "\n",
      "0.816881 (0.004829) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
      "\n",
      "0.816838 (0.004198) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
      "\n",
      "0.816637 (0.005093) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
      "\n",
      "0.816015 (0.003300) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
      "\n",
      "0.815986 (0.005435) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
      "\n",
      "0.815956 (0.004574) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 1500, 'ANN__n2_units': 432, 'ANN__n3_units': 432, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
      "\n",
      "0.815930 (0.003477) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
      "\n",
      "0.815898 (0.002754) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
      "\n",
      "0.815804 (0.004866) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 600, 'ANN__n2_units': 3000, 'ANN__n3_units': 200, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
      "\n",
      "0.815421 (0.003607) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.4, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.3}\n",
      "\n",
      "0.815203 (0.004517) with: {'ANN__activation': 'relu', 'ANN__batch_size': 256, 'ANN__class_weight': [{0: 0.6833225411900137, 1: 1.8637166405023549}, {0: 0.5397840295538505, 1: 6.7839285714285715}, {0: 0.6576760612145973, 1: 2.0855292050944225}, {0: 0.6546388199614006, 1: 2.1166703811009584}, {0: 0.6584055459272097, 1: 2.0782275711159737}, {0: 0.6251645602948921, 1: 2.4973704969760715}, {0: 0.5554093567251462, 1: 5.011873350923483}, {0: 0.6524800769442154, 1: 2.1395584591124126}, {0: 0.517546727698763, 1: 14.747670807453416}, {0: 0.5353418634800744, 1: 7.573763955342903}, {0: 0.5303791813257386, 1: 8.729319852941176}], 'ANN__epochs': 15, 'ANN__hidden_drop_rate': 0.5, 'ANN__init_mode': 'uniform', 'ANN__maxnorm_value': 3, 'ANN__n1_units': 800, 'ANN__n2_units': 1500, 'ANN__n3_units': 300, 'ANN__optimizer': 'Adam', 'ANN__visible_drop_rate': 0.4}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### CLASSIFICATION: SUBTASK 1 & 2\n",
    "\n",
    "# prepare features, disregard 'pid' and 'Time' columns\n",
    "relevant_features = X_train.columns[2:]\n",
    "\n",
    "# feature engineering\n",
    "# For each ‘pid’ and relevant column, we compute the following features: \n",
    "# mean, min, max, difference between min and max, first available (i.e. not nan) observation, \n",
    "# last available observation, difference between first and last, \n",
    "# the number of missing values over all 12 observations. \n",
    "# Whenever there is no observation per ‘pid’ and relevant column, \n",
    "# we impute the value with the mean of that column over the entire dataset.\n",
    "def clf_features(X):\n",
    "    X_mean = X[relevant_features].mean()\n",
    "\n",
    "    X_pid_mean = X.groupby(['pid'], as_index=False)[relevant_features].mean().drop(['pid'], axis=1)\n",
    "    X_pid_mean.fillna({col:X_mean[col] for col in X_pid_mean.columns}, inplace=True)\n",
    "\n",
    "    X_pid_min = X.groupby(['pid'], as_index=False)[relevant_features].min().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_min.fillna({col:X_mean[col] for col in X_pid_min.columns}, inplace=True)\n",
    "\n",
    "    X_pid_max = X.groupby(['pid'], as_index=False)[relevant_features].max().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_max.fillna({col:X_mean[col] for col in X_pid_max.columns}, inplace=True)\n",
    "\n",
    "    X_pid_diff_0 = X_pid_max-X_pid_min\n",
    "\n",
    "    X_pid_first = X.groupby(['pid'], as_index=False)[relevant_features].first().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_first.fillna({col:X_mean[col] for col in X_pid_first.columns}, inplace=True)\n",
    "\n",
    "    X_pid_last = X.groupby(['pid'], as_index=False)[relevant_features].last().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_last.fillna({col:X_mean[col] for col in X_pid_last.columns}, inplace=True)\n",
    "\n",
    "    X_pid_diff_1 = X_pid_last-X_pid_first\n",
    "\n",
    "    X_pid_missing = X.groupby(['pid'], as_index=False)[relevant_features].count().drop(['pid', 'Age'], axis=1)\n",
    "\n",
    "    X_final = pd.concat([X_pid_mean, X_pid_min, X_pid_max, X_pid_diff_0, X_pid_first, \n",
    "                        X_pid_last, X_pid_diff_1, X_pid_missing], axis=1).values\n",
    "    return X_final\n",
    "\n",
    "X_train_clf, X_test_clf = [clf_features(X) for X in [X_train, X_test]]\n",
    "\n",
    "# define function to create an ANN classifier model\n",
    "def create_model(input_dim=1, output_dim=1, n1_units=100, n2_units=100, n3_units=100, \n",
    "                 activation='relu', optimizer='Adam', visible_drop_rate=0.2, hidden_drop_rate=0.5, \n",
    "                 init_mode='glorot_uniform', maxnorm_value=3):\n",
    "    # input layer\n",
    "    visible = Input(shape=(input_dim,))\n",
    "    drop0 = Dropout(visible_drop_rate)(visible)\n",
    "    # hidden layer 1\n",
    "    hidden1 = Dense(n1_units, kernel_initializer=init_mode, kernel_constraint=maxnorm(maxnorm_value))(drop0)\n",
    "    batch1 = BatchNormalization()(hidden1)\n",
    "    act1 = Activation(activation)(batch1)\n",
    "    drop1 = Dropout(hidden_drop_rate)(act1)\n",
    "    # hidden layer 2\n",
    "    hidden2 = Dense(n2_units, kernel_initializer=init_mode, kernel_constraint=maxnorm(maxnorm_value))(drop1)\n",
    "    batch2 = BatchNormalization()(hidden2)\n",
    "    act2 = Activation(activation)(batch2)\n",
    "    drop2 = Dropout(hidden_drop_rate)(act2)\n",
    "    # hidden layer 3\n",
    "    hidden3 = Dense(n3_units, kernel_initializer=init_mode, kernel_constraint=maxnorm(maxnorm_value))(drop2)\n",
    "    batch3 = BatchNormalization()(hidden3)\n",
    "    act3 = Activation(activation)(batch3)\n",
    "    drop3 = Dropout(hidden_drop_rate)(act3)\n",
    "    # output layers\n",
    "    multi_output = [Dense(1, activation='sigmoid', kernel_initializer=init_mode)(drop3) for i in range(output_dim)]\n",
    "    model = Model(inputs=visible, outputs=multi_output)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[keras.metrics.AUC(name='roc_auc')])\n",
    "    return model\n",
    "\n",
    "\n",
    "# create custom sklearn classifier s.t. Keras functional API can be used in Pipeline and GridSearchCV\n",
    "class CustomClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n1_units=100, n2_units=100, n3_units=100, activation='sigmoid', \n",
    "                 optimizer='Adam', visible_drop_rate=0.2, hidden_drop_rate=0.5, \n",
    "                 init_mode='glorot_uniform', maxnorm_value=3,\n",
    "                 batch_size=None, epochs=1, class_weight=None):\n",
    "        self.n1_units = n1_units\n",
    "        self.n2_units = n2_units \n",
    "        self.n3_units = n3_units         \n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.visible_drop_rate = visible_drop_rate\n",
    "        self.hidden_drop_rate = hidden_drop_rate\n",
    "        self.maxnorm_value = maxnorm_value\n",
    "        self.init_mode = init_mode\n",
    "        # model.fit parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.class_weight = class_weight\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, batch_size=None, epochs=1, verbose=0, \n",
    "            validation_data=None, class_weight=None):\n",
    "        # determine output dimension\n",
    "        if len(y.shape)==1:\n",
    "            self.output_dim_ = 1\n",
    "        else:\n",
    "            self.output_dim_ = y.shape[1]\n",
    "        \n",
    "        # reshape target into 2d-array\n",
    "        y_reshaped = y.reshape(y.shape[0], self.output_dim_)\n",
    "        \n",
    "        # create model\n",
    "        self.model_ = create_model(input_dim=X.shape[1], output_dim=self.output_dim_, \n",
    "                                   n1_units=self.n1_units, n2_units=self.n2_units, \n",
    "                                   n3_units=self.n3_units, activation=self.activation, \n",
    "                                   optimizer=self.optimizer, visible_drop_rate=self.visible_drop_rate, \n",
    "                                   hidden_drop_rate=self.hidden_drop_rate, init_mode=self.init_mode, \n",
    "                                   maxnorm_value=self.maxnorm_value)\n",
    "        \n",
    "        # fit parameters entered in fit method \n",
    "        # have priority over the same parameters entered in __init__\n",
    "        if batch_size is not None:\n",
    "            fit_batch_size = batch_size\n",
    "        else:\n",
    "            fit_batch_size = self.batch_size\n",
    "        if epochs is not 1:\n",
    "            fit_epochs = epochs\n",
    "        else: \n",
    "            fit_epochs = self.epochs\n",
    "        if class_weight is not None:\n",
    "            fit_class_weight = class_weight\n",
    "        else: \n",
    "            fit_class_weight = self.class_weight\n",
    "        \n",
    "        # fit model and save history in self.history_ attribute\n",
    "        self.history_ = self.model_.fit(\n",
    "            X, [y_reshaped[:, i] for i in range(self.output_dim_)], \n",
    "            batch_size=fit_batch_size, epochs=fit_epochs, verbose=verbose, \n",
    "            validation_data=validation_data, class_weight=fit_class_weight\n",
    "        )\n",
    "        # return classifier\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        # make prediction\n",
    "        if self.output_dim_ == 1:\n",
    "            predictions = self.model_.predict(X, verbose=0)\n",
    "        else:\n",
    "            predictions = np.concatenate(self.model_.predict(X, verbose=0), axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    # same as predict but predict_proba needs to be defined in order to be able \n",
    "    # to use scoring='roc_auc' in GridSearchCV\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "\n",
    "# define pipeline\n",
    "steps = [('scaler', StandardScaler()), ('ANN', CustomClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# compute class_weight='balanced' as in sklearn.utils.class_weight.compute_class_weight\n",
    "def balanced_class_weight(col_name):\n",
    "    y = y_train[col_name]\n",
    "    class_weight = dict(len(y) / (y.nunique() * y.value_counts()))\n",
    "    return class_weight\n",
    "\n",
    "class_weight = [balanced_class_weight(col_name) for col_name in subtask_1+subtask_2]\n",
    "\n",
    "# define parameter choice for param_grid in GridSearchCV\n",
    "param_grid = [dict(ANN__n1_units=[600], ANN__n2_units=[3000], ANN__n3_units=[200], \n",
    "                  ANN__activation=['relu'], ANN__optimizer=['Adam'], \n",
    "                  ANN__visible_drop_rate=[0.3, 0.4], ANN__hidden_drop_rate=[0.4, 0.5], \n",
    "                  ANN__init_mode=['uniform'], ANN__maxnorm_value=[3], \n",
    "                  ANN__epochs=[15], ANN__batch_size=[256], ANN__class_weight=[class_weight]), \n",
    "              dict(ANN__n1_units=[800], ANN__n2_units=[1500], ANN__n3_units=[300], \n",
    "                  ANN__activation=['relu'], ANN__optimizer=['Adam'], \n",
    "                  ANN__visible_drop_rate=[0.3, 0.4], ANN__hidden_drop_rate=[0.4, 0.5], \n",
    "                  ANN__init_mode=['uniform'], ANN__maxnorm_value=[3], \n",
    "                  ANN__epochs=[15], ANN__batch_size=[256], ANN__class_weight=[class_weight]), \n",
    "              dict(ANN__n1_units=[1500], ANN__n2_units=[432], ANN__n3_units=[432], \n",
    "                  ANN__activation=['relu'], ANN__optimizer=['Adam'], \n",
    "                  ANN__visible_drop_rate=[0.3, 0.4], ANN__hidden_drop_rate=[0.4, 0.5], \n",
    "                  ANN__init_mode=['uniform'], ANN__maxnorm_value=[3], \n",
    "                  ANN__epochs=[15], ANN__batch_size=[256], ANN__class_weight=[class_weight])]\n",
    "                  \n",
    "\n",
    "# define classifier\n",
    "classifier = GridSearchCV(estimator=pipeline, param_grid=param_grid, n_jobs=-1, \n",
    "                          cv=5, verbose=50, scoring='roc_auc')\n",
    "\n",
    "print(\"# perform classifier grid search\")\n",
    "start_time = time.time()\n",
    "classifier.fit(X_train_clf, y_train[subtask_1+subtask_2].values)\n",
    "run_time = time.time()-start_time\n",
    "print('# perform classifier grid search: runtime: {a:.0f} h {b:.0f} min {c:.2f} s'.format(a=run_time//3600, b=(run_time-(run_time//3600)*3600)//60, c=run_time%60))\n",
    "\n",
    "print('# make predictions')\n",
    "df_predictions[subtask_1+subtask_2] = pd.DataFrame(classifier.predict(X_test_clf))\n",
    "\n",
    "print('# summarize GridSearchCV results')\n",
    "cv_results = pd.DataFrame(classifier.cv_results_).sort_values(by=['mean_test_score'], ascending=False).loc[:, ['mean_test_score', 'std_test_score', 'params']]\n",
    "for mean, stdev, param in zip(*[cv_results[col] for col in cv_results]):\n",
    "    print(\"{:f} ({:f}) with: {}\".format(mean, stdev, param))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJHart8IZEQX",
    "outputId": "621f11cc-23b8-44d1-bdf9-10ba0fe6d19d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# perform regression\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    }
   ],
   "source": [
    "#### REGRESSION: SUBTASK 3\n",
    "\n",
    "# prepare features\n",
    "relevant_features = ['Age', 'Temp', 'ABPd', 'ABPs', 'pH', 'Glucose', 'Hgb']+[col[len('LABEL_'):] for col in subtask_3]\n",
    "\n",
    "# same feature engineering as for classification above\n",
    "def regr_features(X):\n",
    "    X_mean = X[relevant_features].mean()\n",
    "\n",
    "    X_pid_mean = X.groupby(['pid'], as_index=False)[relevant_features].mean().drop(['pid'], axis=1)\n",
    "    X_pid_mean.fillna({col:X_mean[col] for col in X_pid_mean.columns}, inplace=True)\n",
    "\n",
    "    X_pid_min = X.groupby(['pid'], as_index=False)[relevant_features].min().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_min.fillna({col:X_mean[col] for col in X_pid_min.columns}, inplace=True)\n",
    "\n",
    "    X_pid_max = X.groupby(['pid'], as_index=False)[relevant_features].max().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_max.fillna({col:X_mean[col] for col in X_pid_max.columns}, inplace=True)\n",
    "\n",
    "    X_pid_diff_0 = X_pid_max-X_pid_min\n",
    "\n",
    "    X_pid_first = X.groupby(['pid'], as_index=False)[relevant_features].first().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_first.fillna({col:X_mean[col] for col in X_pid_first.columns}, inplace=True)\n",
    "\n",
    "    X_pid_last = X.groupby(['pid'], as_index=False)[relevant_features].last().drop(['pid', 'Age'], axis=1)\n",
    "    X_pid_last.fillna({col:X_mean[col] for col in X_pid_last.columns}, inplace=True)\n",
    "\n",
    "    X_pid_diff_1 = X_pid_last-X_pid_first\n",
    "\n",
    "    X_pid_missing = X.groupby(['pid'], as_index=False)[relevant_features].count().drop(['pid', 'Age'], axis=1)\n",
    "\n",
    "    X_regr = pd.concat([X_pid_mean, X_pid_min, X_pid_max, X_pid_diff_0, X_pid_first, \n",
    "                        X_pid_last, X_pid_diff_1, X_pid_missing], axis=1).values\n",
    "    return X_regr\n",
    "\n",
    "X_train_regr, X_test_regr = [regr_features(X) for X in [X_train, X_test]]\n",
    "\n",
    "# create regressor pipeline\n",
    "steps = [('scaler', StandardScaler()), ('regr', Ridge())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# define parameter choice\n",
    "# regr=[Ridge()]\n",
    "alpha = [10**i for i in range(-1, 4)]\n",
    "# regr=[RandomForestRegressor(n_jobs=-1)]\n",
    "max_features = ['sqrt']\n",
    "n_estimators = [300, 400, 600, 700]\n",
    "\n",
    "param_grid = [dict(regr=[Ridge()], \n",
    "                   regr__alpha=alpha), \n",
    "              dict(regr=[RandomForestRegressor()], \n",
    "                   regr__max_features=max_features, \n",
    "                   regr__n_estimators=n_estimators)]\n",
    "\n",
    "# create regressor\n",
    "regressor = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1, cv=5, verbose=50, scoring='r2')\n",
    "\n",
    "print('# perform regression')\n",
    "start_time = time.time()\n",
    "best_scores = []\n",
    "for col_name in subtask_3:\n",
    "    regressor.fit(X_train_regr, y_train[col_name].values)\n",
    "    best_scores.append(regressor.best_score_)\n",
    "    df_predictions[col_name] = regressor.predict(X_test_regr)\n",
    "    print('########### {}: summarize GridSearchCV results'.format(col_name))\n",
    "    cv_results = pd.DataFrame(regressor.cv_results_).sort_values(by=['mean_test_score'], ascending=False).loc[:, ['mean_test_score', 'std_test_score', 'params']]\n",
    "    for mean, stdev, param in zip(*[cv_results[col] for col in cv_results]):\n",
    "        print(\"{:f} ({:f}) with:\".format(mean, stdev))\n",
    "        print(param)\n",
    "        print()\n",
    "\n",
    "print('########### Mean r2-score: {}'.format(np.mean(best_scores)))\n",
    "run_time = time.time()-start_time\n",
    "print('# perform regression: runtime: {a:.0f} h {b:.0f} min {c:.2f} s'.format(a=run_time//3600, b=(run_time-(run_time//3600)*3600)//60, c=run_time%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGKkAbDuZEQY"
   },
   "outputs": [],
   "source": [
    "# df_predictions is a pandas dataframe containing the final result\n",
    "filename = 'Submission_{}.zip'.format(main_filename)\n",
    "df_predictions[['pid']+subtask_1+subtask_2+subtask_3].to_csv(filename, index=False, float_format='%.5f', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
